{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7849c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2703bb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1, 데이터 가져오기\n",
    "load_diabetes()['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91740cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2~3. 모델에 입력할 X, y 데이터 준비\n",
    "df_X = np.array(load_diabetes()['data'])\n",
    "df_y = np.array(load_diabetes()['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "056bb25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(442,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621,  0.02187235, -0.0442235 ,\n",
       "        -0.03482076, -0.04340085, -0.00259226,  0.01990842, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, -0.02632783, -0.00844872,\n",
       "        -0.01916334,  0.07441156, -0.03949338, -0.06832974, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, -0.00567061, -0.04559945,\n",
       "        -0.03419447, -0.03235593, -0.00259226,  0.00286377, -0.02593034],\n",
       "       [-0.08906294, -0.04464164, -0.01159501, -0.03665645,  0.01219057,\n",
       "         0.02499059, -0.03603757,  0.03430886,  0.02269202, -0.00936191],\n",
       "       [ 0.00538306, -0.04464164, -0.03638469,  0.02187235,  0.00393485,\n",
       "         0.01559614,  0.00814208, -0.00259226, -0.03199144, -0.04664087]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_X.shape)\n",
    "display(df_y.shape)\n",
    "display(df_X[:5,:])\n",
    "display(df_y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3e1104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. train 데이터와 test 데이터로 분리하기\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d3b370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 모델 준비하기\n",
    "def model(X, W, b):\n",
    "    predictions = 0\n",
    "    for i in range(10):\n",
    "       predictions += X[:, i]*W[i]\n",
    "    predictions += b\n",
    "    return predictions\n",
    "\n",
    "# 6. 손실함수 loss 정의하기\n",
    "def MSE(a, b):\n",
    "    mse = ((a-b)**2).mean()\n",
    "    return mse\n",
    "\n",
    "def loss(X, W, b, y):\n",
    "    predictions = model(X, W, b)\n",
    "    L = MSE(predictions, y)\n",
    "    return L\n",
    "\n",
    "# 7. 기울기를 구하는 gradient 함수 구현하기\n",
    "def gradient(X, W, b, y):\n",
    "    N = len(y)\n",
    "    y_pred = model(X, W, b)\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10c1326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8~9. 하이퍼 파라미터 학습률 설정 및 모델 학습 수행\n",
    "def model_learning(X, y, n_est, mue):\n",
    "    W = np.random.rand(10)\n",
    "    b = random.random()\n",
    "    losses = []\n",
    "    W_list = [W]\n",
    "    b_list = [b]\n",
    "    \n",
    "    for i in range(1, n_est+1):\n",
    "        dW, db = gradient(X_train, W, b, y_train)\n",
    "        W -= mue * dW\n",
    "        b -= mue * db\n",
    "        L = loss(X_train, W, b, y_train)\n",
    "        losses.append(L)\n",
    "        W_list.append(W)\n",
    "        b_list.append(b)\n",
    "        if i % 10 == 0:\n",
    "            print('Iteration %d : Loss %0.4f' % (i, L))\n",
    "    \n",
    "    return W_list, b_list, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be91fbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 6299.6934\n",
      "Iteration 20 : Loss 5955.0492\n",
      "Iteration 30 : Loss 5870.4462\n",
      "Iteration 40 : Loss 5791.5138\n",
      "Iteration 50 : Loss 5715.2224\n",
      "Iteration 60 : Loss 5641.4437\n",
      "Iteration 70 : Loss 5570.0864\n",
      "Iteration 80 : Loss 5501.0630\n",
      "Iteration 90 : Loss 5434.2891\n",
      "Iteration 100 : Loss 5369.6836\n",
      "Iteration 110 : Loss 5307.1684\n",
      "Iteration 120 : Loss 5246.6681\n",
      "Iteration 130 : Loss 5188.1104\n",
      "Iteration 140 : Loss 5131.4254\n",
      "Iteration 150 : Loss 5076.5460\n",
      "Iteration 160 : Loss 5023.4075\n",
      "Iteration 170 : Loss 4971.9477\n",
      "Iteration 180 : Loss 4922.1064\n",
      "Iteration 190 : Loss 4873.8260\n",
      "Iteration 200 : Loss 4827.0509\n",
      "Iteration 210 : Loss 4781.7274\n",
      "Iteration 220 : Loss 4737.8040\n",
      "Iteration 230 : Loss 4695.2311\n",
      "Iteration 240 : Loss 4653.9608\n",
      "Iteration 250 : Loss 4613.9471\n",
      "Iteration 260 : Loss 4575.1455\n",
      "Iteration 270 : Loss 4537.5133\n",
      "Iteration 280 : Loss 4501.0095\n",
      "Iteration 290 : Loss 4465.5943\n",
      "Iteration 300 : Loss 4431.2296\n",
      "Iteration 310 : Loss 4397.8787\n",
      "Iteration 320 : Loss 4365.5060\n",
      "Iteration 330 : Loss 4334.0776\n",
      "Iteration 340 : Loss 4303.5606\n",
      "Iteration 350 : Loss 4273.9233\n",
      "Iteration 360 : Loss 4245.1352\n",
      "Iteration 370 : Loss 4217.1670\n",
      "Iteration 380 : Loss 4189.9904\n",
      "Iteration 390 : Loss 4163.5781\n",
      "Iteration 400 : Loss 4137.9039\n",
      "Iteration 410 : Loss 4112.9426\n",
      "Iteration 420 : Loss 4088.6697\n",
      "Iteration 430 : Loss 4065.0617\n",
      "Iteration 440 : Loss 4042.0962\n",
      "Iteration 450 : Loss 4019.7512\n",
      "Iteration 460 : Loss 3998.0058\n",
      "Iteration 470 : Loss 3976.8398\n",
      "Iteration 480 : Loss 3956.2336\n",
      "Iteration 490 : Loss 3936.1686\n",
      "Iteration 500 : Loss 3916.6265\n",
      "Iteration 510 : Loss 3897.5900\n",
      "Iteration 520 : Loss 3879.0422\n",
      "Iteration 530 : Loss 3860.9670\n",
      "Iteration 540 : Loss 3843.3488\n",
      "Iteration 550 : Loss 3826.1724\n",
      "Iteration 560 : Loss 3809.4234\n",
      "Iteration 570 : Loss 3793.0879\n",
      "Iteration 580 : Loss 3777.1523\n",
      "Iteration 590 : Loss 3761.6036\n",
      "Iteration 600 : Loss 3746.4295\n",
      "Iteration 610 : Loss 3731.6176\n",
      "Iteration 620 : Loss 3717.1566\n",
      "Iteration 630 : Loss 3703.0350\n",
      "Iteration 640 : Loss 3689.2422\n",
      "Iteration 650 : Loss 3675.7676\n",
      "Iteration 660 : Loss 3662.6012\n",
      "Iteration 670 : Loss 3649.7334\n",
      "Iteration 680 : Loss 3637.1547\n",
      "Iteration 690 : Loss 3624.8562\n",
      "Iteration 700 : Loss 3612.8292\n",
      "Iteration 710 : Loss 3601.0652\n",
      "Iteration 720 : Loss 3589.5562\n",
      "Iteration 730 : Loss 3578.2944\n",
      "Iteration 740 : Loss 3567.2722\n",
      "Iteration 750 : Loss 3556.4823\n",
      "Iteration 760 : Loss 3545.9179\n",
      "Iteration 770 : Loss 3535.5720\n",
      "Iteration 780 : Loss 3525.4382\n",
      "Iteration 790 : Loss 3515.5102\n",
      "Iteration 800 : Loss 3505.7819\n",
      "Iteration 810 : Loss 3496.2475\n",
      "Iteration 820 : Loss 3486.9012\n",
      "Iteration 830 : Loss 3477.7376\n",
      "Iteration 840 : Loss 3468.7515\n",
      "Iteration 850 : Loss 3459.9377\n",
      "Iteration 860 : Loss 3451.2914\n",
      "Iteration 870 : Loss 3442.8077\n",
      "Iteration 880 : Loss 3434.4821\n",
      "Iteration 890 : Loss 3426.3102\n",
      "Iteration 900 : Loss 3418.2878\n",
      "Iteration 910 : Loss 3410.4105\n",
      "Iteration 920 : Loss 3402.6746\n",
      "Iteration 930 : Loss 3395.0761\n",
      "Iteration 940 : Loss 3387.6113\n",
      "Iteration 950 : Loss 3380.2766\n",
      "Iteration 960 : Loss 3373.0686\n",
      "Iteration 970 : Loss 3365.9838\n",
      "Iteration 980 : Loss 3359.0191\n",
      "Iteration 990 : Loss 3352.1712\n",
      "Iteration 1000 : Loss 3345.4372\n",
      "Iteration 1010 : Loss 3338.8142\n",
      "Iteration 1020 : Loss 3332.2992\n",
      "Iteration 1030 : Loss 3325.8895\n",
      "Iteration 1040 : Loss 3319.5826\n",
      "Iteration 1050 : Loss 3313.3758\n",
      "Iteration 1060 : Loss 3307.2666\n",
      "Iteration 1070 : Loss 3301.2527\n",
      "Iteration 1080 : Loss 3295.3317\n",
      "Iteration 1090 : Loss 3289.5014\n",
      "Iteration 1100 : Loss 3283.7596\n",
      "Iteration 1110 : Loss 3278.1041\n",
      "Iteration 1120 : Loss 3272.5331\n",
      "Iteration 1130 : Loss 3267.0444\n",
      "Iteration 1140 : Loss 3261.6361\n",
      "Iteration 1150 : Loss 3256.3065\n",
      "Iteration 1160 : Loss 3251.0538\n",
      "Iteration 1170 : Loss 3245.8761\n",
      "Iteration 1180 : Loss 3240.7718\n",
      "Iteration 1190 : Loss 3235.7393\n",
      "Iteration 1200 : Loss 3230.7770\n",
      "Iteration 1210 : Loss 3225.8833\n",
      "Iteration 1220 : Loss 3221.0569\n",
      "Iteration 1230 : Loss 3216.2962\n",
      "Iteration 1240 : Loss 3211.5998\n",
      "Iteration 1250 : Loss 3206.9664\n",
      "Iteration 1260 : Loss 3202.3947\n",
      "Iteration 1270 : Loss 3197.8834\n",
      "Iteration 1280 : Loss 3193.4312\n",
      "Iteration 1290 : Loss 3189.0370\n",
      "Iteration 1300 : Loss 3184.6997\n",
      "Iteration 1310 : Loss 3180.4179\n",
      "Iteration 1320 : Loss 3176.1908\n",
      "Iteration 1330 : Loss 3172.0172\n",
      "Iteration 1340 : Loss 3167.8960\n",
      "Iteration 1350 : Loss 3163.8263\n",
      "Iteration 1360 : Loss 3159.8071\n",
      "Iteration 1370 : Loss 3155.8374\n",
      "Iteration 1380 : Loss 3151.9163\n",
      "Iteration 1390 : Loss 3148.0430\n",
      "Iteration 1400 : Loss 3144.2164\n",
      "Iteration 1410 : Loss 3140.4359\n",
      "Iteration 1420 : Loss 3136.7005\n",
      "Iteration 1430 : Loss 3133.0095\n",
      "Iteration 1440 : Loss 3129.3621\n",
      "Iteration 1450 : Loss 3125.7575\n",
      "Iteration 1460 : Loss 3122.1950\n",
      "Iteration 1470 : Loss 3118.6738\n",
      "Iteration 1480 : Loss 3115.1932\n",
      "Iteration 1490 : Loss 3111.7527\n",
      "Iteration 1500 : Loss 3108.3514\n",
      "Iteration 1510 : Loss 3104.9888\n",
      "Iteration 1520 : Loss 3101.6642\n",
      "Iteration 1530 : Loss 3098.3770\n",
      "Iteration 1540 : Loss 3095.1266\n",
      "Iteration 1550 : Loss 3091.9124\n",
      "Iteration 1560 : Loss 3088.7338\n",
      "Iteration 1570 : Loss 3085.5904\n",
      "Iteration 1580 : Loss 3082.4814\n",
      "Iteration 1590 : Loss 3079.4065\n",
      "Iteration 1600 : Loss 3076.3650\n",
      "Iteration 1610 : Loss 3073.3565\n",
      "Iteration 1620 : Loss 3070.3805\n",
      "Iteration 1630 : Loss 3067.4365\n",
      "Iteration 1640 : Loss 3064.5240\n",
      "Iteration 1650 : Loss 3061.6426\n",
      "Iteration 1660 : Loss 3058.7918\n",
      "Iteration 1670 : Loss 3055.9712\n",
      "Iteration 1680 : Loss 3053.1803\n",
      "Iteration 1690 : Loss 3050.4187\n",
      "Iteration 1700 : Loss 3047.6860\n",
      "Iteration 1710 : Loss 3044.9818\n",
      "Iteration 1720 : Loss 3042.3057\n",
      "Iteration 1730 : Loss 3039.6574\n",
      "Iteration 1740 : Loss 3037.0364\n",
      "Iteration 1750 : Loss 3034.4423\n",
      "Iteration 1760 : Loss 3031.8749\n",
      "Iteration 1770 : Loss 3029.3337\n",
      "Iteration 1780 : Loss 3026.8184\n",
      "Iteration 1790 : Loss 3024.3287\n",
      "Iteration 1800 : Loss 3021.8642\n",
      "Iteration 1810 : Loss 3019.4246\n",
      "Iteration 1820 : Loss 3017.0096\n",
      "Iteration 1830 : Loss 3014.6189\n",
      "Iteration 1840 : Loss 3012.2521\n",
      "Iteration 1850 : Loss 3009.9089\n",
      "Iteration 1860 : Loss 3007.5892\n",
      "Iteration 1870 : Loss 3005.2924\n",
      "Iteration 1880 : Loss 3003.0185\n",
      "Iteration 1890 : Loss 3000.7671\n",
      "Iteration 1900 : Loss 2998.5378\n",
      "Iteration 1910 : Loss 2996.3306\n",
      "Iteration 1920 : Loss 2994.1450\n",
      "Iteration 1930 : Loss 2991.9808\n",
      "Iteration 1940 : Loss 2989.8378\n",
      "Iteration 1950 : Loss 2987.7158\n",
      "Iteration 1960 : Loss 2985.6143\n",
      "Iteration 1970 : Loss 2983.5334\n",
      "Iteration 1980 : Loss 2981.4725\n",
      "Iteration 1990 : Loss 2979.4317\n",
      "Iteration 2000 : Loss 2977.4105\n",
      "Iteration 2010 : Loss 2975.4089\n",
      "Iteration 2020 : Loss 2973.4265\n",
      "Iteration 2030 : Loss 2971.4631\n",
      "Iteration 2040 : Loss 2969.5186\n",
      "Iteration 2050 : Loss 2967.5927\n",
      "Iteration 2060 : Loss 2965.6852\n",
      "Iteration 2070 : Loss 2963.7959\n",
      "Iteration 2080 : Loss 2961.9246\n",
      "Iteration 2090 : Loss 2960.0711\n",
      "Iteration 2100 : Loss 2958.2352\n",
      "Iteration 2110 : Loss 2956.4167\n",
      "Iteration 2120 : Loss 2954.6154\n",
      "Iteration 2130 : Loss 2952.8311\n",
      "Iteration 2140 : Loss 2951.0636\n",
      "Iteration 2150 : Loss 2949.3128\n",
      "Iteration 2160 : Loss 2947.5785\n",
      "Iteration 2170 : Loss 2945.8605\n",
      "Iteration 2180 : Loss 2944.1586\n",
      "Iteration 2190 : Loss 2942.4726\n",
      "Iteration 2200 : Loss 2940.8025\n",
      "Iteration 2210 : Loss 2939.1479\n",
      "Iteration 2220 : Loss 2937.5088\n",
      "Iteration 2230 : Loss 2935.8850\n",
      "Iteration 2240 : Loss 2934.2762\n",
      "Iteration 2250 : Loss 2932.6825\n",
      "Iteration 2260 : Loss 2931.1036\n",
      "Iteration 2270 : Loss 2929.5393\n",
      "Iteration 2280 : Loss 2927.9895\n",
      "Iteration 2290 : Loss 2926.4540\n",
      "Iteration 2300 : Loss 2924.9328\n",
      "Iteration 2310 : Loss 2923.4256\n",
      "Iteration 2320 : Loss 2921.9323\n",
      "Iteration 2330 : Loss 2920.4528\n",
      "Iteration 2340 : Loss 2918.9869\n",
      "Iteration 2350 : Loss 2917.5345\n",
      "Iteration 2360 : Loss 2916.0954\n",
      "Iteration 2370 : Loss 2914.6696\n",
      "Iteration 2380 : Loss 2913.2568\n",
      "Iteration 2390 : Loss 2911.8570\n",
      "Iteration 2400 : Loss 2910.4700\n",
      "Iteration 2410 : Loss 2909.0957\n",
      "Iteration 2420 : Loss 2907.7340\n",
      "Iteration 2430 : Loss 2906.3846\n",
      "Iteration 2440 : Loss 2905.0476\n",
      "Iteration 2450 : Loss 2903.7228\n",
      "Iteration 2460 : Loss 2902.4101\n",
      "Iteration 2470 : Loss 2901.1093\n",
      "Iteration 2480 : Loss 2899.8203\n",
      "Iteration 2490 : Loss 2898.5430\n",
      "Iteration 2500 : Loss 2897.2774\n",
      "Iteration 2510 : Loss 2896.0232\n",
      "Iteration 2520 : Loss 2894.7804\n",
      "Iteration 2530 : Loss 2893.5488\n",
      "Iteration 2540 : Loss 2892.3284\n",
      "Iteration 2550 : Loss 2891.1190\n",
      "Iteration 2560 : Loss 2889.9206\n",
      "Iteration 2570 : Loss 2888.7330\n",
      "Iteration 2580 : Loss 2887.5560\n",
      "Iteration 2590 : Loss 2886.3897\n",
      "Iteration 2600 : Loss 2885.2340\n",
      "Iteration 2610 : Loss 2884.0886\n",
      "Iteration 2620 : Loss 2882.9535\n",
      "Iteration 2630 : Loss 2881.8286\n",
      "Iteration 2640 : Loss 2880.7139\n",
      "Iteration 2650 : Loss 2879.6091\n",
      "Iteration 2660 : Loss 2878.5143\n",
      "Iteration 2670 : Loss 2877.4293\n",
      "Iteration 2680 : Loss 2876.3540\n",
      "Iteration 2690 : Loss 2875.2884\n",
      "Iteration 2700 : Loss 2874.2322\n",
      "Iteration 2710 : Loss 2873.1856\n",
      "Iteration 2720 : Loss 2872.1483\n",
      "Iteration 2730 : Loss 2871.1202\n",
      "Iteration 2740 : Loss 2870.1014\n",
      "Iteration 2750 : Loss 2869.0916\n",
      "Iteration 2760 : Loss 2868.0908\n",
      "Iteration 2770 : Loss 2867.0990\n",
      "Iteration 2780 : Loss 2866.1160\n",
      "Iteration 2790 : Loss 2865.1417\n",
      "Iteration 2800 : Loss 2864.1762\n",
      "Iteration 2810 : Loss 2863.2192\n",
      "Iteration 2820 : Loss 2862.2707\n",
      "Iteration 2830 : Loss 2861.3306\n",
      "Iteration 2840 : Loss 2860.3989\n",
      "Iteration 2850 : Loss 2859.4754\n",
      "Iteration 2860 : Loss 2858.5602\n",
      "Iteration 2870 : Loss 2857.6530\n",
      "Iteration 2880 : Loss 2856.7539\n",
      "Iteration 2890 : Loss 2855.8628\n",
      "Iteration 2900 : Loss 2854.9795\n",
      "Iteration 2910 : Loss 2854.1041\n",
      "Iteration 2920 : Loss 2853.2363\n",
      "Iteration 2930 : Loss 2852.3763\n",
      "Iteration 2940 : Loss 2851.5238\n",
      "Iteration 2950 : Loss 2850.6789\n",
      "Iteration 2960 : Loss 2849.8414\n",
      "Iteration 2970 : Loss 2849.0113\n",
      "Iteration 2980 : Loss 2848.1886\n",
      "Iteration 2990 : Loss 2847.3730\n",
      "Iteration 3000 : Loss 2846.5647\n",
      "Iteration 3010 : Loss 2845.7635\n",
      "Iteration 3020 : Loss 2844.9693\n",
      "Iteration 3030 : Loss 2844.1821\n",
      "Iteration 3040 : Loss 2843.4018\n",
      "Iteration 3050 : Loss 2842.6283\n",
      "Iteration 3060 : Loss 2841.8617\n",
      "Iteration 3070 : Loss 2841.1018\n",
      "Iteration 3080 : Loss 2840.3485\n",
      "Iteration 3090 : Loss 2839.6019\n",
      "Iteration 3100 : Loss 2838.8617\n",
      "Iteration 3110 : Loss 2838.1281\n",
      "Iteration 3120 : Loss 2837.4009\n",
      "Iteration 3130 : Loss 2836.6800\n",
      "Iteration 3140 : Loss 2835.9655\n",
      "Iteration 3150 : Loss 2835.2572\n",
      "Iteration 3160 : Loss 2834.5551\n",
      "Iteration 3170 : Loss 2833.8591\n",
      "Iteration 3180 : Loss 2833.1692\n",
      "Iteration 3190 : Loss 2832.4853\n",
      "Iteration 3200 : Loss 2831.8074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3210 : Loss 2831.1354\n",
      "Iteration 3220 : Loss 2830.4692\n",
      "Iteration 3230 : Loss 2829.8088\n",
      "Iteration 3240 : Loss 2829.1542\n",
      "Iteration 3250 : Loss 2828.5053\n",
      "Iteration 3260 : Loss 2827.8620\n",
      "Iteration 3270 : Loss 2827.2243\n",
      "Iteration 3280 : Loss 2826.5922\n",
      "Iteration 3290 : Loss 2825.9655\n",
      "Iteration 3300 : Loss 2825.3443\n",
      "Iteration 3310 : Loss 2824.7285\n",
      "Iteration 3320 : Loss 2824.1180\n",
      "Iteration 3330 : Loss 2823.5128\n",
      "Iteration 3340 : Loss 2822.9128\n",
      "Iteration 3350 : Loss 2822.3181\n",
      "Iteration 3360 : Loss 2821.7284\n",
      "Iteration 3370 : Loss 2821.1439\n",
      "Iteration 3380 : Loss 2820.5645\n",
      "Iteration 3390 : Loss 2819.9900\n",
      "Iteration 3400 : Loss 2819.4206\n",
      "Iteration 3410 : Loss 2818.8560\n",
      "Iteration 3420 : Loss 2818.2963\n",
      "Iteration 3430 : Loss 2817.7414\n",
      "Iteration 3440 : Loss 2817.1914\n",
      "Iteration 3450 : Loss 2816.6460\n",
      "Iteration 3460 : Loss 2816.1054\n",
      "Iteration 3470 : Loss 2815.5694\n",
      "Iteration 3480 : Loss 2815.0381\n",
      "Iteration 3490 : Loss 2814.5113\n",
      "Iteration 3500 : Loss 2813.9890\n",
      "Iteration 3510 : Loss 2813.4713\n",
      "Iteration 3520 : Loss 2812.9579\n",
      "Iteration 3530 : Loss 2812.4490\n",
      "Iteration 3540 : Loss 2811.9445\n",
      "Iteration 3550 : Loss 2811.4443\n",
      "Iteration 3560 : Loss 2810.9484\n",
      "Iteration 3570 : Loss 2810.4567\n",
      "Iteration 3580 : Loss 2809.9693\n",
      "Iteration 3590 : Loss 2809.4860\n",
      "Iteration 3600 : Loss 2809.0069\n",
      "Iteration 3610 : Loss 2808.5318\n",
      "Iteration 3620 : Loss 2808.0609\n",
      "Iteration 3630 : Loss 2807.5939\n",
      "Iteration 3640 : Loss 2807.1310\n",
      "Iteration 3650 : Loss 2806.6720\n",
      "Iteration 3660 : Loss 2806.2169\n",
      "Iteration 3670 : Loss 2805.7658\n",
      "Iteration 3680 : Loss 2805.3184\n",
      "Iteration 3690 : Loss 2804.8749\n",
      "Iteration 3700 : Loss 2804.4352\n",
      "Iteration 3710 : Loss 2803.9992\n",
      "Iteration 3720 : Loss 2803.5669\n",
      "Iteration 3730 : Loss 2803.1383\n",
      "Iteration 3740 : Loss 2802.7134\n",
      "Iteration 3750 : Loss 2802.2921\n",
      "Iteration 3760 : Loss 2801.8743\n",
      "Iteration 3770 : Loss 2801.4601\n",
      "Iteration 3780 : Loss 2801.0494\n",
      "Iteration 3790 : Loss 2800.6423\n",
      "Iteration 3800 : Loss 2800.2385\n",
      "Iteration 3810 : Loss 2799.8382\n",
      "Iteration 3820 : Loss 2799.4413\n",
      "Iteration 3830 : Loss 2799.0478\n",
      "Iteration 3840 : Loss 2798.6576\n",
      "Iteration 3850 : Loss 2798.2707\n",
      "Iteration 3860 : Loss 2797.8870\n",
      "Iteration 3870 : Loss 2797.5067\n",
      "Iteration 3880 : Loss 2797.1295\n",
      "Iteration 3890 : Loss 2796.7555\n",
      "Iteration 3900 : Loss 2796.3847\n",
      "Iteration 3910 : Loss 2796.0170\n",
      "Iteration 3920 : Loss 2795.6525\n",
      "Iteration 3930 : Loss 2795.2910\n",
      "Iteration 3940 : Loss 2794.9325\n",
      "Iteration 3950 : Loss 2794.5771\n",
      "Iteration 3960 : Loss 2794.2247\n",
      "Iteration 3970 : Loss 2793.8752\n",
      "Iteration 3980 : Loss 2793.5287\n",
      "Iteration 3990 : Loss 2793.1851\n",
      "Iteration 4000 : Loss 2792.8444\n",
      "Iteration 4010 : Loss 2792.5065\n",
      "Iteration 4020 : Loss 2792.1715\n",
      "Iteration 4030 : Loss 2791.8393\n",
      "Iteration 4040 : Loss 2791.5099\n",
      "Iteration 4050 : Loss 2791.1833\n",
      "Iteration 4060 : Loss 2790.8594\n",
      "Iteration 4070 : Loss 2790.5382\n",
      "Iteration 4080 : Loss 2790.2197\n",
      "Iteration 4090 : Loss 2789.9039\n",
      "Iteration 4100 : Loss 2789.5908\n",
      "Iteration 4110 : Loss 2789.2802\n",
      "Iteration 4120 : Loss 2788.9723\n",
      "Iteration 4130 : Loss 2788.6669\n",
      "Iteration 4140 : Loss 2788.3641\n",
      "Iteration 4150 : Loss 2788.0638\n",
      "Iteration 4160 : Loss 2787.7660\n",
      "Iteration 4170 : Loss 2787.4707\n",
      "Iteration 4180 : Loss 2787.1779\n",
      "Iteration 4190 : Loss 2786.8875\n",
      "Iteration 4200 : Loss 2786.5996\n",
      "Iteration 4210 : Loss 2786.3140\n",
      "Iteration 4220 : Loss 2786.0308\n",
      "Iteration 4230 : Loss 2785.7500\n",
      "Iteration 4240 : Loss 2785.4715\n",
      "Iteration 4250 : Loss 2785.1953\n",
      "Iteration 4260 : Loss 2784.9214\n",
      "Iteration 4270 : Loss 2784.6498\n",
      "Iteration 4280 : Loss 2784.3805\n",
      "Iteration 4290 : Loss 2784.1134\n",
      "Iteration 4300 : Loss 2783.8485\n",
      "Iteration 4310 : Loss 2783.5858\n",
      "Iteration 4320 : Loss 2783.3253\n",
      "Iteration 4330 : Loss 2783.0669\n",
      "Iteration 4340 : Loss 2782.8107\n",
      "Iteration 4350 : Loss 2782.5566\n",
      "Iteration 4360 : Loss 2782.3046\n",
      "Iteration 4370 : Loss 2782.0547\n",
      "Iteration 4380 : Loss 2781.8069\n",
      "Iteration 4390 : Loss 2781.5611\n",
      "Iteration 4400 : Loss 2781.3174\n",
      "Iteration 4410 : Loss 2781.0756\n",
      "Iteration 4420 : Loss 2780.8359\n",
      "Iteration 4430 : Loss 2780.5981\n",
      "Iteration 4440 : Loss 2780.3623\n",
      "Iteration 4450 : Loss 2780.1284\n",
      "Iteration 4460 : Loss 2779.8965\n",
      "Iteration 4470 : Loss 2779.6664\n",
      "Iteration 4480 : Loss 2779.4383\n",
      "Iteration 4490 : Loss 2779.2120\n",
      "Iteration 4500 : Loss 2778.9876\n",
      "Iteration 4510 : Loss 2778.7651\n",
      "Iteration 4520 : Loss 2778.5443\n",
      "Iteration 4530 : Loss 2778.3254\n",
      "Iteration 4540 : Loss 2778.1083\n",
      "Iteration 4550 : Loss 2777.8930\n",
      "Iteration 4560 : Loss 2777.6794\n",
      "Iteration 4570 : Loss 2777.4676\n",
      "Iteration 4580 : Loss 2777.2575\n",
      "Iteration 4590 : Loss 2777.0491\n",
      "Iteration 4600 : Loss 2776.8425\n",
      "Iteration 4610 : Loss 2776.6375\n",
      "Iteration 4620 : Loss 2776.4342\n",
      "Iteration 4630 : Loss 2776.2326\n",
      "Iteration 4640 : Loss 2776.0326\n",
      "Iteration 4650 : Loss 2775.8342\n",
      "Iteration 4660 : Loss 2775.6375\n",
      "Iteration 4670 : Loss 2775.4423\n",
      "Iteration 4680 : Loss 2775.2488\n",
      "Iteration 4690 : Loss 2775.0568\n",
      "Iteration 4700 : Loss 2774.8664\n",
      "Iteration 4710 : Loss 2774.6776\n",
      "Iteration 4720 : Loss 2774.4903\n",
      "Iteration 4730 : Loss 2774.3045\n",
      "Iteration 4740 : Loss 2774.1202\n",
      "Iteration 4750 : Loss 2773.9374\n",
      "Iteration 4760 : Loss 2773.7561\n",
      "Iteration 4770 : Loss 2773.5763\n",
      "Iteration 4780 : Loss 2773.3979\n",
      "Iteration 4790 : Loss 2773.2210\n",
      "Iteration 4800 : Loss 2773.0455\n",
      "Iteration 4810 : Loss 2772.8714\n",
      "Iteration 4820 : Loss 2772.6987\n",
      "Iteration 4830 : Loss 2772.5275\n",
      "Iteration 4840 : Loss 2772.3576\n",
      "Iteration 4850 : Loss 2772.1891\n",
      "Iteration 4860 : Loss 2772.0219\n",
      "Iteration 4870 : Loss 2771.8561\n",
      "Iteration 4880 : Loss 2771.6917\n",
      "Iteration 4890 : Loss 2771.5286\n",
      "Iteration 4900 : Loss 2771.3667\n",
      "Iteration 4910 : Loss 2771.2062\n",
      "Iteration 4920 : Loss 2771.0470\n",
      "Iteration 4930 : Loss 2770.8891\n",
      "Iteration 4940 : Loss 2770.7324\n",
      "Iteration 4950 : Loss 2770.5770\n",
      "Iteration 4960 : Loss 2770.4228\n",
      "Iteration 4970 : Loss 2770.2699\n",
      "Iteration 4980 : Loss 2770.1182\n",
      "Iteration 4990 : Loss 2769.9677\n",
      "Iteration 5000 : Loss 2769.8185\n",
      "Iteration 5010 : Loss 2769.6704\n",
      "Iteration 5020 : Loss 2769.5235\n",
      "Iteration 5030 : Loss 2769.3778\n",
      "Iteration 5040 : Loss 2769.2333\n",
      "Iteration 5050 : Loss 2769.0899\n",
      "Iteration 5060 : Loss 2768.9476\n",
      "Iteration 5070 : Loss 2768.8065\n",
      "Iteration 5080 : Loss 2768.6666\n",
      "Iteration 5090 : Loss 2768.5277\n",
      "Iteration 5100 : Loss 2768.3899\n",
      "Iteration 5110 : Loss 2768.2533\n",
      "Iteration 5120 : Loss 2768.1177\n",
      "Iteration 5130 : Loss 2767.9832\n",
      "Iteration 5140 : Loss 2767.8498\n",
      "Iteration 5150 : Loss 2767.7175\n",
      "Iteration 5160 : Loss 2767.5862\n",
      "Iteration 5170 : Loss 2767.4559\n",
      "Iteration 5180 : Loss 2767.3267\n",
      "Iteration 5190 : Loss 2767.1985\n",
      "Iteration 5200 : Loss 2767.0713\n",
      "Iteration 5210 : Loss 2766.9452\n",
      "Iteration 5220 : Loss 2766.8200\n",
      "Iteration 5230 : Loss 2766.6958\n",
      "Iteration 5240 : Loss 2766.5726\n",
      "Iteration 5250 : Loss 2766.4504\n",
      "Iteration 5260 : Loss 2766.3292\n",
      "Iteration 5270 : Loss 2766.2089\n",
      "Iteration 5280 : Loss 2766.0895\n",
      "Iteration 5290 : Loss 2765.9711\n",
      "Iteration 5300 : Loss 2765.8537\n",
      "Iteration 5310 : Loss 2765.7371\n",
      "Iteration 5320 : Loss 2765.6215\n",
      "Iteration 5330 : Loss 2765.5068\n",
      "Iteration 5340 : Loss 2765.3930\n",
      "Iteration 5350 : Loss 2765.2801\n",
      "Iteration 5360 : Loss 2765.1680\n",
      "Iteration 5370 : Loss 2765.0569\n",
      "Iteration 5380 : Loss 2764.9466\n",
      "Iteration 5390 : Loss 2764.8372\n",
      "Iteration 5400 : Loss 2764.7287\n",
      "Iteration 5410 : Loss 2764.6210\n",
      "Iteration 5420 : Loss 2764.5141\n",
      "Iteration 5430 : Loss 2764.4081\n",
      "Iteration 5440 : Loss 2764.3030\n",
      "Iteration 5450 : Loss 2764.1986\n",
      "Iteration 5460 : Loss 2764.0951\n",
      "Iteration 5470 : Loss 2763.9923\n",
      "Iteration 5480 : Loss 2763.8904\n",
      "Iteration 5490 : Loss 2763.7893\n",
      "Iteration 5500 : Loss 2763.6889\n",
      "Iteration 5510 : Loss 2763.5893\n",
      "Iteration 5520 : Loss 2763.4906\n",
      "Iteration 5530 : Loss 2763.3925\n",
      "Iteration 5540 : Loss 2763.2953\n",
      "Iteration 5550 : Loss 2763.1988\n",
      "Iteration 5560 : Loss 2763.1030\n",
      "Iteration 5570 : Loss 2763.0080\n",
      "Iteration 5580 : Loss 2762.9138\n",
      "Iteration 5590 : Loss 2762.8202\n",
      "Iteration 5600 : Loss 2762.7274\n",
      "Iteration 5610 : Loss 2762.6353\n",
      "Iteration 5620 : Loss 2762.5440\n",
      "Iteration 5630 : Loss 2762.4533\n",
      "Iteration 5640 : Loss 2762.3633\n",
      "Iteration 5650 : Loss 2762.2741\n",
      "Iteration 5660 : Loss 2762.1855\n",
      "Iteration 5670 : Loss 2762.0976\n",
      "Iteration 5680 : Loss 2762.0104\n",
      "Iteration 5690 : Loss 2761.9238\n",
      "Iteration 5700 : Loss 2761.8380\n",
      "Iteration 5710 : Loss 2761.7527\n",
      "Iteration 5720 : Loss 2761.6682\n",
      "Iteration 5730 : Loss 2761.5843\n",
      "Iteration 5740 : Loss 2761.5010\n",
      "Iteration 5750 : Loss 2761.4184\n",
      "Iteration 5760 : Loss 2761.3364\n",
      "Iteration 5770 : Loss 2761.2550\n",
      "Iteration 5780 : Loss 2761.1743\n",
      "Iteration 5790 : Loss 2761.0942\n",
      "Iteration 5800 : Loss 2761.0147\n",
      "Iteration 5810 : Loss 2760.9358\n",
      "Iteration 5820 : Loss 2760.8575\n",
      "Iteration 5830 : Loss 2760.7798\n",
      "Iteration 5840 : Loss 2760.7027\n",
      "Iteration 5850 : Loss 2760.6262\n",
      "Iteration 5860 : Loss 2760.5503\n",
      "Iteration 5870 : Loss 2760.4749\n",
      "Iteration 5880 : Loss 2760.4002\n",
      "Iteration 5890 : Loss 2760.3260\n",
      "Iteration 5900 : Loss 2760.2523\n",
      "Iteration 5910 : Loss 2760.1793\n",
      "Iteration 5920 : Loss 2760.1068\n",
      "Iteration 5930 : Loss 2760.0348\n",
      "Iteration 5940 : Loss 2759.9634\n",
      "Iteration 5950 : Loss 2759.8925\n",
      "Iteration 5960 : Loss 2759.8221\n",
      "Iteration 5970 : Loss 2759.7523\n",
      "Iteration 5980 : Loss 2759.6831\n",
      "Iteration 5990 : Loss 2759.6143\n",
      "Iteration 6000 : Loss 2759.5461\n",
      "Iteration 6010 : Loss 2759.4784\n",
      "Iteration 6020 : Loss 2759.4112\n",
      "Iteration 6030 : Loss 2759.3445\n",
      "Iteration 6040 : Loss 2759.2783\n",
      "Iteration 6050 : Loss 2759.2126\n",
      "Iteration 6060 : Loss 2759.1474\n",
      "Iteration 6070 : Loss 2759.0827\n",
      "Iteration 6080 : Loss 2759.0184\n",
      "Iteration 6090 : Loss 2758.9547\n",
      "Iteration 6100 : Loss 2758.8914\n",
      "Iteration 6110 : Loss 2758.8286\n",
      "Iteration 6120 : Loss 2758.7663\n",
      "Iteration 6130 : Loss 2758.7045\n",
      "Iteration 6140 : Loss 2758.6431\n",
      "Iteration 6150 : Loss 2758.5822\n",
      "Iteration 6160 : Loss 2758.5217\n",
      "Iteration 6170 : Loss 2758.4617\n",
      "Iteration 6180 : Loss 2758.4021\n",
      "Iteration 6190 : Loss 2758.3430\n",
      "Iteration 6200 : Loss 2758.2843\n",
      "Iteration 6210 : Loss 2758.2261\n",
      "Iteration 6220 : Loss 2758.1683\n",
      "Iteration 6230 : Loss 2758.1109\n",
      "Iteration 6240 : Loss 2758.0539\n",
      "Iteration 6250 : Loss 2757.9974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6260 : Loss 2757.9413\n",
      "Iteration 6270 : Loss 2757.8856\n",
      "Iteration 6280 : Loss 2757.8303\n",
      "Iteration 6290 : Loss 2757.7755\n",
      "Iteration 6300 : Loss 2757.7210\n",
      "Iteration 6310 : Loss 2757.6669\n",
      "Iteration 6320 : Loss 2757.6133\n",
      "Iteration 6330 : Loss 2757.5600\n",
      "Iteration 6340 : Loss 2757.5072\n",
      "Iteration 6350 : Loss 2757.4547\n",
      "Iteration 6360 : Loss 2757.4026\n",
      "Iteration 6370 : Loss 2757.3509\n",
      "Iteration 6380 : Loss 2757.2996\n",
      "Iteration 6390 : Loss 2757.2486\n",
      "Iteration 6400 : Loss 2757.1980\n",
      "Iteration 6410 : Loss 2757.1478\n",
      "Iteration 6420 : Loss 2757.0980\n",
      "Iteration 6430 : Loss 2757.0486\n",
      "Iteration 6440 : Loss 2756.9995\n",
      "Iteration 6450 : Loss 2756.9507\n",
      "Iteration 6460 : Loss 2756.9023\n",
      "Iteration 6470 : Loss 2756.8543\n",
      "Iteration 6480 : Loss 2756.8066\n",
      "Iteration 6490 : Loss 2756.7593\n",
      "Iteration 6500 : Loss 2756.7123\n",
      "Iteration 6510 : Loss 2756.6657\n",
      "Iteration 6520 : Loss 2756.6194\n",
      "Iteration 6530 : Loss 2756.5734\n",
      "Iteration 6540 : Loss 2756.5278\n",
      "Iteration 6550 : Loss 2756.4825\n",
      "Iteration 6560 : Loss 2756.4375\n",
      "Iteration 6570 : Loss 2756.3929\n",
      "Iteration 6580 : Loss 2756.3485\n",
      "Iteration 6590 : Loss 2756.3045\n",
      "Iteration 6600 : Loss 2756.2609\n",
      "Iteration 6610 : Loss 2756.2175\n",
      "Iteration 6620 : Loss 2756.1745\n",
      "Iteration 6630 : Loss 2756.1317\n",
      "Iteration 6640 : Loss 2756.0893\n",
      "Iteration 6650 : Loss 2756.0472\n",
      "Iteration 6660 : Loss 2756.0054\n",
      "Iteration 6670 : Loss 2755.9638\n",
      "Iteration 6680 : Loss 2755.9226\n",
      "Iteration 6690 : Loss 2755.8817\n",
      "Iteration 6700 : Loss 2755.8411\n",
      "Iteration 6710 : Loss 2755.8007\n",
      "Iteration 6720 : Loss 2755.7607\n",
      "Iteration 6730 : Loss 2755.7209\n",
      "Iteration 6740 : Loss 2755.6815\n",
      "Iteration 6750 : Loss 2755.6423\n",
      "Iteration 6760 : Loss 2755.6034\n",
      "Iteration 6770 : Loss 2755.5648\n",
      "Iteration 6780 : Loss 2755.5264\n",
      "Iteration 6790 : Loss 2755.4883\n",
      "Iteration 6800 : Loss 2755.4505\n",
      "Iteration 6810 : Loss 2755.4130\n",
      "Iteration 6820 : Loss 2755.3757\n",
      "Iteration 6830 : Loss 2755.3387\n",
      "Iteration 6840 : Loss 2755.3019\n",
      "Iteration 6850 : Loss 2755.2655\n",
      "Iteration 6860 : Loss 2755.2292\n",
      "Iteration 6870 : Loss 2755.1933\n",
      "Iteration 6880 : Loss 2755.1576\n",
      "Iteration 6890 : Loss 2755.1221\n",
      "Iteration 6900 : Loss 2755.0869\n",
      "Iteration 6910 : Loss 2755.0519\n",
      "Iteration 6920 : Loss 2755.0172\n",
      "Iteration 6930 : Loss 2754.9828\n",
      "Iteration 6940 : Loss 2754.9485\n",
      "Iteration 6950 : Loss 2754.9146\n",
      "Iteration 6960 : Loss 2754.8808\n",
      "Iteration 6970 : Loss 2754.8473\n",
      "Iteration 6980 : Loss 2754.8140\n",
      "Iteration 6990 : Loss 2754.7810\n",
      "Iteration 7000 : Loss 2754.7482\n",
      "Iteration 7010 : Loss 2754.7156\n",
      "Iteration 7020 : Loss 2754.6833\n",
      "Iteration 7030 : Loss 2754.6511\n",
      "Iteration 7040 : Loss 2754.6192\n",
      "Iteration 7050 : Loss 2754.5876\n",
      "Iteration 7060 : Loss 2754.5561\n",
      "Iteration 7070 : Loss 2754.5249\n",
      "Iteration 7080 : Loss 2754.4939\n",
      "Iteration 7090 : Loss 2754.4631\n",
      "Iteration 7100 : Loss 2754.4325\n",
      "Iteration 7110 : Loss 2754.4021\n",
      "Iteration 7120 : Loss 2754.3719\n",
      "Iteration 7130 : Loss 2754.3420\n",
      "Iteration 7140 : Loss 2754.3122\n",
      "Iteration 7150 : Loss 2754.2827\n",
      "Iteration 7160 : Loss 2754.2534\n",
      "Iteration 7170 : Loss 2754.2242\n",
      "Iteration 7180 : Loss 2754.1953\n",
      "Iteration 7190 : Loss 2754.1666\n",
      "Iteration 7200 : Loss 2754.1380\n",
      "Iteration 7210 : Loss 2754.1097\n",
      "Iteration 7220 : Loss 2754.0815\n",
      "Iteration 7230 : Loss 2754.0536\n",
      "Iteration 7240 : Loss 2754.0258\n",
      "Iteration 7250 : Loss 2753.9983\n",
      "Iteration 7260 : Loss 2753.9709\n",
      "Iteration 7270 : Loss 2753.9437\n",
      "Iteration 7280 : Loss 2753.9167\n",
      "Iteration 7290 : Loss 2753.8899\n",
      "Iteration 7300 : Loss 2753.8632\n",
      "Iteration 7310 : Loss 2753.8368\n",
      "Iteration 7320 : Loss 2753.8105\n",
      "Iteration 7330 : Loss 2753.7844\n",
      "Iteration 7340 : Loss 2753.7585\n",
      "Iteration 7350 : Loss 2753.7327\n",
      "Iteration 7360 : Loss 2753.7071\n",
      "Iteration 7370 : Loss 2753.6817\n",
      "Iteration 7380 : Loss 2753.6565\n",
      "Iteration 7390 : Loss 2753.6315\n",
      "Iteration 7400 : Loss 2753.6066\n",
      "Iteration 7410 : Loss 2753.5819\n",
      "Iteration 7420 : Loss 2753.5573\n",
      "Iteration 7430 : Loss 2753.5329\n",
      "Iteration 7440 : Loss 2753.5087\n",
      "Iteration 7450 : Loss 2753.4846\n",
      "Iteration 7460 : Loss 2753.4608\n",
      "Iteration 7470 : Loss 2753.4370\n",
      "Iteration 7480 : Loss 2753.4134\n",
      "Iteration 7490 : Loss 2753.3900\n",
      "Iteration 7500 : Loss 2753.3668\n",
      "Iteration 7510 : Loss 2753.3436\n",
      "Iteration 7520 : Loss 2753.3207\n",
      "Iteration 7530 : Loss 2753.2979\n",
      "Iteration 7540 : Loss 2753.2752\n",
      "Iteration 7550 : Loss 2753.2528\n",
      "Iteration 7560 : Loss 2753.2304\n",
      "Iteration 7570 : Loss 2753.2082\n",
      "Iteration 7580 : Loss 2753.1862\n",
      "Iteration 7590 : Loss 2753.1642\n",
      "Iteration 7600 : Loss 2753.1425\n",
      "Iteration 7610 : Loss 2753.1209\n",
      "Iteration 7620 : Loss 2753.0994\n",
      "Iteration 7630 : Loss 2753.0781\n",
      "Iteration 7640 : Loss 2753.0569\n",
      "Iteration 7650 : Loss 2753.0358\n",
      "Iteration 7660 : Loss 2753.0149\n",
      "Iteration 7670 : Loss 2752.9941\n",
      "Iteration 7680 : Loss 2752.9735\n",
      "Iteration 7690 : Loss 2752.9530\n",
      "Iteration 7700 : Loss 2752.9326\n",
      "Iteration 7710 : Loss 2752.9124\n",
      "Iteration 7720 : Loss 2752.8923\n",
      "Iteration 7730 : Loss 2752.8723\n",
      "Iteration 7740 : Loss 2752.8525\n",
      "Iteration 7750 : Loss 2752.8328\n",
      "Iteration 7760 : Loss 2752.8132\n",
      "Iteration 7770 : Loss 2752.7937\n",
      "Iteration 7780 : Loss 2752.7744\n",
      "Iteration 7790 : Loss 2752.7552\n",
      "Iteration 7800 : Loss 2752.7361\n",
      "Iteration 7810 : Loss 2752.7171\n",
      "Iteration 7820 : Loss 2752.6983\n",
      "Iteration 7830 : Loss 2752.6796\n",
      "Iteration 7840 : Loss 2752.6610\n",
      "Iteration 7850 : Loss 2752.6425\n",
      "Iteration 7860 : Loss 2752.6242\n",
      "Iteration 7870 : Loss 2752.6059\n",
      "Iteration 7880 : Loss 2752.5878\n",
      "Iteration 7890 : Loss 2752.5698\n",
      "Iteration 7900 : Loss 2752.5519\n",
      "Iteration 7910 : Loss 2752.5341\n",
      "Iteration 7920 : Loss 2752.5165\n",
      "Iteration 7930 : Loss 2752.4989\n",
      "Iteration 7940 : Loss 2752.4815\n",
      "Iteration 7950 : Loss 2752.4642\n",
      "Iteration 7960 : Loss 2752.4470\n",
      "Iteration 7970 : Loss 2752.4299\n",
      "Iteration 7980 : Loss 2752.4129\n",
      "Iteration 7990 : Loss 2752.3960\n",
      "Iteration 8000 : Loss 2752.3792\n",
      "Iteration 8010 : Loss 2752.3625\n",
      "Iteration 8020 : Loss 2752.3460\n",
      "Iteration 8030 : Loss 2752.3295\n",
      "Iteration 8040 : Loss 2752.3131\n",
      "Iteration 8050 : Loss 2752.2969\n",
      "Iteration 8060 : Loss 2752.2807\n",
      "Iteration 8070 : Loss 2752.2647\n",
      "Iteration 8080 : Loss 2752.2487\n",
      "Iteration 8090 : Loss 2752.2329\n",
      "Iteration 8100 : Loss 2752.2171\n",
      "Iteration 8110 : Loss 2752.2014\n",
      "Iteration 8120 : Loss 2752.1859\n",
      "Iteration 8130 : Loss 2752.1704\n",
      "Iteration 8140 : Loss 2752.1551\n",
      "Iteration 8150 : Loss 2752.1398\n",
      "Iteration 8160 : Loss 2752.1246\n",
      "Iteration 8170 : Loss 2752.1095\n",
      "Iteration 8180 : Loss 2752.0945\n",
      "Iteration 8190 : Loss 2752.0797\n",
      "Iteration 8200 : Loss 2752.0649\n",
      "Iteration 8210 : Loss 2752.0501\n",
      "Iteration 8220 : Loss 2752.0355\n",
      "Iteration 8230 : Loss 2752.0210\n",
      "Iteration 8240 : Loss 2752.0065\n",
      "Iteration 8250 : Loss 2751.9922\n",
      "Iteration 8260 : Loss 2751.9779\n",
      "Iteration 8270 : Loss 2751.9638\n",
      "Iteration 8280 : Loss 2751.9497\n",
      "Iteration 8290 : Loss 2751.9357\n",
      "Iteration 8300 : Loss 2751.9217\n",
      "Iteration 8310 : Loss 2751.9079\n",
      "Iteration 8320 : Loss 2751.8942\n",
      "Iteration 8330 : Loss 2751.8805\n",
      "Iteration 8340 : Loss 2751.8669\n",
      "Iteration 8350 : Loss 2751.8534\n",
      "Iteration 8360 : Loss 2751.8400\n",
      "Iteration 8370 : Loss 2751.8266\n",
      "Iteration 8380 : Loss 2751.8134\n",
      "Iteration 8390 : Loss 2751.8002\n",
      "Iteration 8400 : Loss 2751.7871\n",
      "Iteration 8410 : Loss 2751.7741\n",
      "Iteration 8420 : Loss 2751.7611\n",
      "Iteration 8430 : Loss 2751.7483\n",
      "Iteration 8440 : Loss 2751.7355\n",
      "Iteration 8450 : Loss 2751.7228\n",
      "Iteration 8460 : Loss 2751.7102\n",
      "Iteration 8470 : Loss 2751.6976\n",
      "Iteration 8480 : Loss 2751.6851\n",
      "Iteration 8490 : Loss 2751.6727\n",
      "Iteration 8500 : Loss 2751.6604\n",
      "Iteration 8510 : Loss 2751.6481\n",
      "Iteration 8520 : Loss 2751.6359\n",
      "Iteration 8530 : Loss 2751.6238\n",
      "Iteration 8540 : Loss 2751.6117\n",
      "Iteration 8550 : Loss 2751.5998\n",
      "Iteration 8560 : Loss 2751.5879\n",
      "Iteration 8570 : Loss 2751.5760\n",
      "Iteration 8580 : Loss 2751.5643\n",
      "Iteration 8590 : Loss 2751.5526\n",
      "Iteration 8600 : Loss 2751.5409\n",
      "Iteration 8610 : Loss 2751.5294\n",
      "Iteration 8620 : Loss 2751.5179\n",
      "Iteration 8630 : Loss 2751.5064\n",
      "Iteration 8640 : Loss 2751.4951\n",
      "Iteration 8650 : Loss 2751.4838\n",
      "Iteration 8660 : Loss 2751.4726\n",
      "Iteration 8670 : Loss 2751.4614\n",
      "Iteration 8680 : Loss 2751.4503\n",
      "Iteration 8690 : Loss 2751.4393\n",
      "Iteration 8700 : Loss 2751.4283\n",
      "Iteration 8710 : Loss 2751.4174\n",
      "Iteration 8720 : Loss 2751.4065\n",
      "Iteration 8730 : Loss 2751.3958\n",
      "Iteration 8740 : Loss 2751.3850\n",
      "Iteration 8750 : Loss 2751.3744\n",
      "Iteration 8760 : Loss 2751.3638\n",
      "Iteration 8770 : Loss 2751.3532\n",
      "Iteration 8780 : Loss 2751.3428\n",
      "Iteration 8790 : Loss 2751.3323\n",
      "Iteration 8800 : Loss 2751.3220\n",
      "Iteration 8810 : Loss 2751.3117\n",
      "Iteration 8820 : Loss 2751.3014\n",
      "Iteration 8830 : Loss 2751.2913\n",
      "Iteration 8840 : Loss 2751.2811\n",
      "Iteration 8850 : Loss 2751.2711\n",
      "Iteration 8860 : Loss 2751.2610\n",
      "Iteration 8870 : Loss 2751.2511\n",
      "Iteration 8880 : Loss 2751.2412\n",
      "Iteration 8890 : Loss 2751.2313\n",
      "Iteration 8900 : Loss 2751.2216\n",
      "Iteration 8910 : Loss 2751.2118\n",
      "Iteration 8920 : Loss 2751.2021\n",
      "Iteration 8930 : Loss 2751.1925\n",
      "Iteration 8940 : Loss 2751.1829\n",
      "Iteration 8950 : Loss 2751.1734\n",
      "Iteration 8960 : Loss 2751.1640\n",
      "Iteration 8970 : Loss 2751.1545\n",
      "Iteration 8980 : Loss 2751.1452\n",
      "Iteration 8990 : Loss 2751.1359\n",
      "Iteration 9000 : Loss 2751.1266\n",
      "Iteration 9010 : Loss 2751.1174\n",
      "Iteration 9020 : Loss 2751.1082\n",
      "Iteration 9030 : Loss 2751.0991\n",
      "Iteration 9040 : Loss 2751.0901\n",
      "Iteration 9050 : Loss 2751.0811\n",
      "Iteration 9060 : Loss 2751.0721\n",
      "Iteration 9070 : Loss 2751.0632\n",
      "Iteration 9080 : Loss 2751.0543\n",
      "Iteration 9090 : Loss 2751.0455\n",
      "Iteration 9100 : Loss 2751.0367\n",
      "Iteration 9110 : Loss 2751.0280\n",
      "Iteration 9120 : Loss 2751.0193\n",
      "Iteration 9130 : Loss 2751.0107\n",
      "Iteration 9140 : Loss 2751.0021\n",
      "Iteration 9150 : Loss 2750.9936\n",
      "Iteration 9160 : Loss 2750.9851\n",
      "Iteration 9170 : Loss 2750.9767\n",
      "Iteration 9180 : Loss 2750.9683\n",
      "Iteration 9190 : Loss 2750.9599\n",
      "Iteration 9200 : Loss 2750.9516\n",
      "Iteration 9210 : Loss 2750.9434\n",
      "Iteration 9220 : Loss 2750.9351\n",
      "Iteration 9230 : Loss 2750.9270\n",
      "Iteration 9240 : Loss 2750.9188\n",
      "Iteration 9250 : Loss 2750.9107\n",
      "Iteration 9260 : Loss 2750.9027\n",
      "Iteration 9270 : Loss 2750.8947\n",
      "Iteration 9280 : Loss 2750.8867\n",
      "Iteration 9290 : Loss 2750.8788\n",
      "Iteration 9300 : Loss 2750.8709\n",
      "Iteration 9310 : Loss 2750.8631\n",
      "Iteration 9320 : Loss 2750.8553\n",
      "Iteration 9330 : Loss 2750.8475\n",
      "Iteration 9340 : Loss 2750.8398\n",
      "Iteration 9350 : Loss 2750.8321\n",
      "Iteration 9360 : Loss 2750.8245\n",
      "Iteration 9370 : Loss 2750.8169\n",
      "Iteration 9380 : Loss 2750.8093\n",
      "Iteration 9390 : Loss 2750.8018\n",
      "Iteration 9400 : Loss 2750.7943\n",
      "Iteration 9410 : Loss 2750.7869\n",
      "Iteration 9420 : Loss 2750.7795\n",
      "Iteration 9430 : Loss 2750.7721\n",
      "Iteration 9440 : Loss 2750.7648\n",
      "Iteration 9450 : Loss 2750.7575\n",
      "Iteration 9460 : Loss 2750.7503\n",
      "Iteration 9470 : Loss 2750.7430\n",
      "Iteration 9480 : Loss 2750.7359\n",
      "Iteration 9490 : Loss 2750.7287\n",
      "Iteration 9500 : Loss 2750.7216\n",
      "Iteration 9510 : Loss 2750.7145\n",
      "Iteration 9520 : Loss 2750.7075\n",
      "Iteration 9530 : Loss 2750.7005\n",
      "Iteration 9540 : Loss 2750.6935\n",
      "Iteration 9550 : Loss 2750.6866\n",
      "Iteration 9560 : Loss 2750.6797\n",
      "Iteration 9570 : Loss 2750.6728\n",
      "Iteration 9580 : Loss 2750.6660\n",
      "Iteration 9590 : Loss 2750.6592\n",
      "Iteration 9600 : Loss 2750.6524\n",
      "Iteration 9610 : Loss 2750.6457\n",
      "Iteration 9620 : Loss 2750.6390\n",
      "Iteration 9630 : Loss 2750.6324\n",
      "Iteration 9640 : Loss 2750.6257\n",
      "Iteration 9650 : Loss 2750.6191\n",
      "Iteration 9660 : Loss 2750.6126\n",
      "Iteration 9670 : Loss 2750.6060\n",
      "Iteration 9680 : Loss 2750.5995\n",
      "Iteration 9690 : Loss 2750.5931\n",
      "Iteration 9700 : Loss 2750.5866\n",
      "Iteration 9710 : Loss 2750.5802\n",
      "Iteration 9720 : Loss 2750.5738\n",
      "Iteration 9730 : Loss 2750.5675\n",
      "Iteration 9740 : Loss 2750.5612\n",
      "Iteration 9750 : Loss 2750.5549\n",
      "Iteration 9760 : Loss 2750.5486\n",
      "Iteration 9770 : Loss 2750.5424\n",
      "Iteration 9780 : Loss 2750.5362\n",
      "Iteration 9790 : Loss 2750.5301\n",
      "Iteration 9800 : Loss 2750.5239\n",
      "Iteration 9810 : Loss 2750.5178\n",
      "Iteration 9820 : Loss 2750.5117\n",
      "Iteration 9830 : Loss 2750.5057\n",
      "Iteration 9840 : Loss 2750.4997\n",
      "Iteration 9850 : Loss 2750.4937\n",
      "Iteration 9860 : Loss 2750.4877\n",
      "Iteration 9870 : Loss 2750.4818\n",
      "Iteration 9880 : Loss 2750.4759\n",
      "Iteration 9890 : Loss 2750.4700\n",
      "Iteration 9900 : Loss 2750.4641\n",
      "Iteration 9910 : Loss 2750.4583\n",
      "Iteration 9920 : Loss 2750.4525\n",
      "Iteration 9930 : Loss 2750.4468\n",
      "Iteration 9940 : Loss 2750.4410\n",
      "Iteration 9950 : Loss 2750.4353\n",
      "Iteration 9960 : Loss 2750.4296\n",
      "Iteration 9970 : Loss 2750.4239\n",
      "Iteration 9980 : Loss 2750.4183\n",
      "Iteration 9990 : Loss 2750.4127\n",
      "Iteration 10000 : Loss 2750.4071\n"
     ]
    }
   ],
   "source": [
    "W_df, b_df, losses_df = model_learning(X_train, y_train, n_est = 10000, mue = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fabca456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a5db74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss가 최소일 때 계수 값 도출\n",
    "def get_min_vel(W_L, b_L, L_L):\n",
    "    L_L_min = min(L_L)\n",
    "    num = L_L.index(L_L_min)\n",
    "    W_L_min = W_L[num+1]\n",
    "    b_L_min = b_L[num+1]\n",
    "    return W_L_min, b_L_min, L_L_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54f53308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -29.78651209 -239.42223148  568.48062641  299.2032039   -77.42936252\n",
      " -150.3169209  -225.87633375  120.26512267  485.56287297   60.50438872]\n",
      "152.38023550266453\n",
      "2750.4070886631575\n"
     ]
    }
   ],
   "source": [
    "W_min, b_min, losses_min = get_min_vel(W_df, b_df, losses_df)\n",
    "print(W_min)\n",
    "print(b_min)\n",
    "print(losses_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b5dacb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3451.106492581432"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(X_test, W_min, b_min)\n",
    "mse = loss(X_test, W_min, b_min, y_test)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7c669ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAphElEQVR4nO2df5QddZXgPzedDnRGoRMTMemEDbpRD0YgQ2RYw87BsBh+KGTURHRW44+ceFb8AeMJtKODIUdPmrBj1JmjTgZnheMgZFFDMCqDCe4MOSJ0JAkJyhJAlzQhJEqimJbudL77R9WLrztV9V7V+1Z9v1Xvfs7p0+99X1W9++rVu3W/997vvWKMQVEURWkPxrkWQFEURSkOVfqKoihthCp9RVGUNkKVvqIoShuhSl9RFKWNGO9aAIApU6aYWbNmuRZDURSlVGzbtu2gMWZqmn28UPqzZs2iv7/ftRiKoiilQkR+nXYfde8oiqK0Ear0FUVR2ghV+oqiKG2EKn1FUZQ2QpW+oihKG+FF9o6iKH6y4ZEBbr73cZ49NMj07i5WLHwdi+b2uBZLaQFV+oqiRLLhkQE+/d1HGRweAWDg0CCf/u6jAKr4S4y6dxRFieTmex8/rvBrDA6PcPO9jzuSSLGBWvpKZVHXRGs8e2gw1bhSDtTSVypJzTUxcGgQw59cExseGXAtWmmY3t2ValwpB6r0lUqironWWbHwdXR1dowa6+rsYMXC1zmSSLGBuneUSqKuidapucLURVYtVOkrlWR6dxcDEQpeXRPpWDS3R5V8xVD3jlJJ1DWhKNGopa9UEnVNKEo0qvSVyqKuCUU5EXXvKIqitBGq9BVFUdoIVfqKoihthPr0E9Bl/IqiVI2Glr6InCwiD4nIDhHZLSI3huNniMjPRGSPiNwpIhPC8ZPC53vC12fl/BlyQZfxK4pSRZpx77wELDDGnA2cA1wiIucDNwFrjTH/GXgB+HC4/YeBF8LxteF2pUOX8SuKUkUaKn0T8GL4tDP8M8AC4K5w/FZgUfj4yvA54esXiYjYErgodBm/ogQz3vl9WzijdxPz+7boTLcCNBXIFZEOEdkOPA/cBzwJHDLGHA032QvUnN09wDMA4euHgVdEHHO5iPSLSP+BAwda+hB5oBUGlXZHXZzVpCmlb4wZMcacA8wAzgNe3+obG2PWGWPmGWPmTZ06tdXDWUeX8Svtjro4q0mq7B1jzCERuR/4L0C3iIwPrfkZQO32PwDMBPaKyHjgVOA3FmUuBF3Gr7Q76uKsJg2VvohMBYZDhd8FXEwQnL0feBdwB7AUuDvcZWP4/Kfh61uMMSYH2XNHl/Er7YxWKq0mzbh3pgH3i8hO4GHgPmPM94Hrgb8RkT0EPvtvhNt/A3hFOP43QK99sRVFyRt1cVaThpa+MWYnMDdi/CkC//7Y8T8Ci61IpyiKM9TFWU10Ra6iKLGoi7N6aO0dRVGUNkKVvqIoShuh7h1FaRO0gKACqvQVpS2ora6tLbaqra4FVPG3Gar0FaUNSFpdu2huj84C2ghV+orSBiStrtVZQHuhgVxFaQOSCghqjZ32QpW+ouSAbyWJk1bXao2d9kLdO4oSYsuv7aO7JGl17c33Pq41dtoIVfqKgl1F3Sho6oq41bUrFr5u1GcHrbFTZdS9oyjYrR1fNnfJork9rH7HG+np7kKAnu4uVr/jjRrErShq6SsKdhV11pLELtMmtcZO+6CWvqJgtz1mlpLE2ppQKQpV+oqC3drxWdwlmjapFIW6dxQF+7Xj07pLyhYHUMqLKn1FCXHp19bWhDmzcz1sXgWH98KpM+CiG+CsJa6lcoK6dxTFA7Q1YY7sXA/3fAIOPwOY4P89nwjGLeHbYrwkVOkrigdo2mSObF4Fw2NmUcODwbgFyhaEV/eOoniCpk3mxOG96cZT4utivDjU0lcUpdqcOiPdeErKFoRXpa8oNXauh7VzYGV38N+iz1dxyEU3QOeYgHhnVzBuAZtrPIpAlb6iQCHBPiWa3IOgZy2Bt38FTp0JSPD/7V+xlr1TtiC8GGNcy8C8efNMf3+/azGUdmbtnFDhj+HUmXDtruLlaRPGFrqDQGGWLYjtqoSGiGwzxsxLs48GchUFcg/2KdGULQgaR5mC8Kr0FQWCoF6kpW8n2JcHVehrW7YgaBVo6NMXkZkicr+IPCYiu0Xkk+H4ShEZEJHt4d9ldft8WkT2iMjjIrIwzw+gKFbIOdhnm7LlhsdRtiBoFWgmkHsU+JQx5kzgfOBqETkzfG2tMeac8O8HAOFrVwFvAC4BvioiHVEHVhRvyDnYZ5uqFGgrWxC0CjR07xhj9gH7wse/F5FfAElzyCuBO4wxLwFPi8ge4DzgpxbkVZT8OGtJeiXvqKZLVdwitgvdKY1J5dMXkVnAXOBnwHzgYyLyfqCfYDbwAsEN4cG63faSfJNQlHJSS/OsLfGvpXlC7oq/SgXayhQErQJN5+mLyMuA7wDXGGN+B3wNeA1wDsFM4O/TvLGILBeRfhHpP3DgQJpdFcUPcq7pkoS6RZSsNGXpi0gngcL/V2PMdwGMMfvrXv9n4Pvh0wFgZt3uM8KxURhj1gHrIMjTzyK8ojjFYZpnklukClk9Sn40VPoiIsA3gF8YY75YNz4t9PcD/BVQW8GyEbhdRL4ITAdmAw9ZlVpRfMBxmmeUW2TsYqdaVk9te0Vpxr0zH3gfsGBMeuYaEXlURHYCbwGuBTDG7AbWA48BPwKuNsaMxBxbUcqLh2meVcnqKR0lqtvUTPbOA4BEvPSDhH2+AHyhBbkUxX9qwVqPOjJVJaunVDgM6GdBV+QqSitkSfPMkSpl9ZSGpIC+R9dGDa2yqSgVwnpWT4ncFs4oWd0mtfQVpUJYXezkqdvCu+ykktVt0tLKilIkjlbwZsLDctNelmIee3OEIKBfQBkPLa2stCdlUaSNLGffPoeHbgvXpZijZxn+BfSTUKWvlBtPXRCRNFrB69vn8NBt4TI7KXkNhF8B/SQ0kKuUmyRF6lsQMslydljSIRYP1yG4LMVsew1E7m0iY1Clr5SbWEX6jH89b+Ms5FNneOlK8bHctMuaQzZnGS77IajSV8pNnCKVjnJZzkk3BJectSQI2q48FPx37MJYNLeH1e94Iz3dXQjQ091VWBDX5izD5cpp9ekr5eaiG6IzJ8Yq/BquLWeID/hFfQ5PO3e5xFUp5hULXxeZOZRlluEyNqFKXyk3cYp08yrvgpBA/ApeD0s6KKOxuQbC5cppzdNXqonD3GlFaYSt9Qaap68oNdRyVjzGZZtItfQVRVFKilr6iqL4SQGrjRNr8vi22tkhqvQVRcmXAlZNb3hkgBV37WB4JPBcDBwaZMVdOwBY1LHVv9XODtE8fUVpAVerKktFAauNb7xn93GFf/wtRgw33rPbz9XODlFLX1Eyov1om6SA1cYvHBmOHz/m4Wpnh6ilrygZ0X60TeJ6tbHr9/cMVfqKG3wrhpYB7UfbJAUUbpOoLt61cQ8Lx7lElb5SPLXAnk/F0DLgsuJjqSigcFtc5rkxxbx/mVCfvlI8JWskHYfNWiyVJ+cG8j0xZQ16ajdgzxrYu0QtfaV4fCwjnAGXFR+V0WhD+OZRS18pHg87MmXFVcXHwijJoqZ2aAhvC1X6SvHElUNu08BaYaRV4CVTftZuwBVxP8ah7h2leDSwVjxZguftuqipIu7HONTSV9yggbViyWK9Vlz5xVIh92MUDS19EZkpIveLyGMisltEPhmOTxaR+0TkifD/pHBcROQrIrJHRHaKyJ/nIbguf1eUFGRR4O26qKnief3NuHeOAp8yxpwJnA9cLSJnAr3AZmPMbGBz+BzgUmB2+Lcc+JptoV02Fc5MhbMBlBKQRYFXXPlBjPFYcfdjQ/eOMWYfsC98/HsR+QXQA1wJXBhudivwE+D6cPw2ExTqf1BEukVkWngcKyQtf/cyk6JkAbHSERegLEnmSRE8/JqPM2fbZ+mSoeNjg2YCu17zcd4Ut1PFG9Ek106qrvsxlU9fRGYBc4GfAafVKfLngNPCxz1AvUNsbzg2SumLyHKCmQCnn356KqFLt/w9iz9VFVZzxN1Q/9+DsOP2StxoE+vEN8k1j83m3OFlXDd+PdPlNzxrXsGao0vY9thstl6RsGOFYy+lMx4t0bTSF5GXAd8BrjHG/E7qil0YY4yIpGrBZYxZB6yDoHNWmn1dNhXORFp/qs4Mmifuhrrtm2BGThx3nHaXVoHbquT57KFBBriAjUMXjBqX2u+oDY2M0hmPlmgqZVNEOgkU/r8aY74bDu8XkWnh69OA58PxAWBm3e4zwjFrWF99lzdp/antmiqXhbgb51iF32j7AsgSi7JVyTOxTlBFaiGlpV1rJzWTvSPAN4BfGGO+WPfSRmBp+HgpcHfd+PvDLJ7zgcM2/flQwuXvaQNi7Zoql4W4G6d0RI87zDzJosBtWaOJhlKCkVHlLLnSGY+WaMa9Mx94H/CoiGwPx/4W6APWi8iHgV8DtbngD4DLgD3AEeCDNgWuUarl72kDYhXPE7ZK3Ores9872qdfG3eYeZJFgdtyZSaWKbg72pgwh/dWukmM1dINJaKZ7J0HgJhq1VwUsb0Brm5RruqRJiDWDmUKbPmQk26op5/vlZ86iwK3Wckz1lCKMTL2M6Xygc5SGY+WEBNXiLpA5s2bZ/r7+12L4RdVDqyNDVRDcFMrKhfa0bkdG5SFQIE3ck3ayN5JJOb7+OQfPoiBMOPnIM+aKaw5uoR7jl3A032X23t/JTMiss0YMy/NPlqGwVeqkioXpWALKmgVqSw7tsLdV8NImK9++JngOeR+vrO6E3K3RmNmS5M27ua64a8yMcztnyEH6eu8hcmdEwCLSr9MBk6ZZI1BLX0lP+Is+rEK/zgCKw9Zees4q3r7yR/hpOGI9+iaDNc/beW9vSWlwjpy0+uZOHhiDsaRrmlMvP6X9mRyOetLg4eyZrH0tcqmkh9xFn0BmTVxmTITohQ+wOBvrb23l2RIy5w4+Fyq8UyUKT25TLImoO4dxQqRrpSkHPqxFr/lQHVsRowhPi0hgdz96nmTxaVWRBaZ6/TkNLMf17JaQi19pWXiFh0d6XpV9A61AlY5FrSKy4g5LC+P3qFrcuyxSlngbyxZFFYRBdcKquQZud4g7eynIlVHVekrJ5B2QU6cK2XN8LvjlcZZS+DaXYEP/9pd1n2icQtvnvjzv4NxnaM3HtcJl94Ueyxbq2Kd0kBhOas2WcCNJdYo+eEN6dw1Fak6qu4dZRRZar3EuVJuffE8Vr73DU6yHeIyZd409xKYNSmVTKWr0RLlskhY+1FItck4N0oBlTzjbtonx8Um4mY/Fak6qtk7yijm922JXEDU093F1t4F1vYpE6X6fEkZJhCpsHL/fI6zXs7o3USUlntgwieYMe7giS+cOjOYfZYAzd5RWiaLVVu2GiZp3Vel+nyNArYRLrXcZzKOs17i4ju3TPjvlXDXpEWVvjKKLJUHy1QAL0tQtkyfL0vANu9qkybmvePGbRN30z7n8uWV7pAVh/r0lVFkrfVSlhomWRtnlOXzZUmztFnfJ4r9TOFVHIgZt0hM3CB5JXRFVr6nQJW+MopGpQJ8zFdPI1PpgrJpyVCsL+9qk6uHFrO685bj5RwAjpgJrB5ezJetvAMNGw/lfdP28XcRhyp95QTifiAbHhngge99lTu5g+knHeTZI1P40veuAj7q7AJPm23kvOta3rVbMmaY5KkU+0+5mN7fcWKrxlMutvcmBdVzisJWd7OiUKXvKT5aDts3rWOVrBtVgGuVWceaTeNZNPdGJzKlddfk7cpIpKg2mJ4V6wvO+dCoVo1dnR2sznrOo26cDlfLlq3XrgZyPcTXFaDLhr41aooOMFGGWDb0LUcSpXfXOA3K2s5i2bke1s6Bld3Bf0/bG1o953GraLsmRW9fwGrZsrkM1dL3EF8th+njfpNqvAiyuGucBWVtWqO2Zw05u52snfO4G+f4rtzrOcXh3GWYErX0PcRXy+GPMbV04sYhfU58WjLl0LuykG3WbrE5a/C1MXrU9xR3gxx8wVn6ZanWcaCWvpc4txxirL6Jl67i6N0fZ/zIH49verTjZCZeGq1oighwpc48KcqvHoXNNpg2Zw02g6C2Zgxx31PXpOgy2KfOcBbLKFuvXVX6HuJzsHE8jPpRj0/4URflpkrlOnCY5WG1dovNsse2biA2b6geunGSKM06DlTpe4lTy6GRUkxhTXnppkpScEW0wstijcYVUNvwUTg2/KftxnVmU362biA2b6hJbpx5H4Jt3wz6MkgHnP1er7KVfEeVvqdUIdjo3E0VRZyC65rkZ2A0zno++70gY7rBjH3eLLbcTjZdTknf047bA4UPwf8dt8Pp53sZwPYRDeQqo2kUbEwRBPUywBVXEx38DIzGWc/bvvmn5u41RoayyWurbr7NQHXZvqcSoUpfGU1So4iUPxIvC5XFKbjBF6K3tx0YTUtSy8k02zfCRlObhGsndRZXzPdkYr6nTMXbKtLzNi3q3lFGkxRsXDsntc/WywBXlF998yr/AqO194+SSzqiFb/L1n0x186GkfnZsrgivqf93/3b+OJtaV01Fel5mxa19KuErfzzOKuvyj8Sm63winBznPsBP2vBR1w7NttNrh5azBEzYdTYETOBfzt6dnpXTUV63qalodIXkX8RkedFZFfd2EoRGRCR7eHfZXWvfVpE9ojI4yKyMC/BC6Eky9yBYvyTFfmR5N4P1uYNJE6ut32xNLXgbWZx9Z9yMb3Dy9h7bArHjLD32BR6h5fx1vE70rtqbLqjSkTDdoki8pfAi8Btxpg54dhK4EVjzP8cs+2ZwLeB84DpwI+B1xoT54AM8LJdouMWb6lZOyfGPWGx9VvZzkkEYxeMQVj8y3asoQ2zQuKw2Y4x7vt7rOMqJLIpogSzjjgivqex7qjaeziPR0WQpV1iQ5++MebfRWRWk8e7ErjDGPMS8LSI7CG4Afw0jVBe4HIRTxaKcL1UoDF0YXWNilgdWpIbi83FhnFrWOQnGdcaRHxPN/dt8bL2lS1aCeR+TETeD/QDnzLGvAD0AA/WbbM3HDsBEVkOLAc4/fTTWxAjJ1z7r9P+oG2u0EzCs7K9afFywVgWXJaTSIntxYaRyQEd9kpcVOYaiSFrIPdrwGuAc4B9wN+nPYAxZp0xZp4xZt7UqVMzipEjLv3XWfzzNv3IFSbvfrCFUbJ0w0Vze9jau4Cn+y5na+8C+xazxZhMZa6RGDIpfWPMfmPMiDHmGPDPBC4cgAFgZt2mM8Ix++QdZHWpRLP8oG0GIiuMlwvGsuB6JhqD0wCojbUG5HCNeJYQksm9IyLTjDH7wqd/BdQihRuB20XkiwSB3NnAQy1LOZYiprYu/ddRbpqk8RoFuF5sdfRy1RmsbBURYynKnZcCH9tpZsHqNeKhG66Z7J1vAxcCU4D9wOfC5+cABvgV8JHaTUBEPgN8CDgKXGOM+WEjIVJn7xSRqeKSGydHL7yRDvhcRFnZgkjMfOnY2vQNsrAMmiqzc31kmevxV/6DM2Wy8vOf47rhr57QAH1N50dZ+dkC2mn6GNjOWVfllb3znojhbyRs/wXgC2mESI2nU1trxGW4Jme+5k5c5sv2TetYJP/UtDXja2ewMrFhZD4PDC/jGu443mz8S8eu4oKR+SxyJNOyoW8xcVxcO82clb6HFnUgh3+6qpxlGDyc2lrl1Jnx1oFD4rIXlg19C8ZFxyA2jMw/YZpc9eyIIrj53scZGHozd/HmUeM/dXjjdNpO09cUaw91VTnLMJQwUyVVgMvTzxeb1RDzozaH90Y2eO+e2Jnq+MqJ+HjjzNJO0xpZ42B54+FvuZxKv2SZKjUf9ljlF6v4Pf18cVkNcT/q/UyJdOMYQ7bsCM+yIFwSd+OMGy+CiZeu4mjHyaPGktppWkU60o0XhYe/5XK6d6BUi4Qy+bA9/HxxWQ0TO1ZFLoxZ/YfFkcc5PDjM2nefky47wlefrSPi8i8a5GXkS8p2mlbxNA4GePdbLq/SLxE+TsWzsqhjK4tOWgUn74WTZgQrIWPSW/t/MAViOmelLrnsq8/WEYcHh1ONN8RW5osrBdcgDuYqRTgRR9lGqvQLwMu2gVloZG2PuWBXjESnZmZa5OJhFoRLrF5TVZhFXXRDdAprWDEzUz3/PHF4zsvp0y8ZlVkFmnKlsNXOWRUp62wLq9dUhhXgvpUe3jAyP7Lkci17zFY9f2s4LKOhln4BVGYVaAZr21rnLFvNuyuC1Wsq5ffqo+WclMJaiHu1RF27VOkXhJdtA9PiMue4AmWdbWPrmjrS9SomDu6LHo/YvqjFdWn88EmKPXf3ahZXjcPfkrp3lOZxnXNsqaCWMpo1w++ObEG4ZvjdkdsXYTmnTXNOqoyZu3s1i6vG4W9Jlb7SPB7mHCutc+uL50X6w2998bzI7YsoPZzWD5+k2K3GlqLI4qpx+FtS946SDs9yjrPgZfqeQ6Z3d7Hx0AVsHLpg1HhPjBK32QkrjrSziUYxjlzdq1ldNY5+S6r0lbbCehAyZQDPxxtOWiVeRGJCFj+8s7hZyZIMVOmXER9LyJYEq0HIlAG8hjccR99rFiWet4ItYjZhjZIlGTSsp18EqevptzNjFQ0EVoX61pvijN5NRF3xAjzdd3m6g6WslT6/b0uk9drT3cXWyw7q9zoGH2dFvpFLPX3FM7QcQUtYTd9LGcBL9FPr93oClUhz9hDN3ikbWo6gJaym76VcJZyY9aLfa75ohdbjqNIvG1qOoCWspu+lzLVOvOHo95ofNZfo4WcA86fYS5sqfnXvlI2SZQr4SJzbILUPOWUALzFg2uH4e61ycoC6zkahgdwyUuUfqCO8aNbu6nutenLAym6IC9+vPFSsLJbJEshVpa8oNMis6V3gQKICSZmFBJTL8Ej4fBsuvLfUGUJZlL769BWFajW6SU3a/rJl85HHxF4efs3H07UxrQiq9BWFYurJeEva/rIOa8FnIqbOzTWPzfavzn4BaCBXUSjZClDLGDOCpBgvZXppRJ2bZ2/fFLlp1Wd3qvShXP5JdKViHhTV6MbH724/U3kVB2LGI3DZV8EilWljmhJV+p72B41TDj52LaoKea8A9fW7Wz20mNWdtzBRho6PHTETWD28mC9H7VDCtOGo31O7zu7Up++hfzKpgYSX/T6VpvD1u+s/5eLIevr9p1wcvUNRteAtraKN+z0B+dbZ95SGlr6I/AvwNuB5Y8yccGwycCcwC/gVsMQY84KICPBl4DLgCPABY8zP8xHdDubw3mh/Zsx4ESQph7bOMik5zx4a5IpxD3Dd+PVMl4M8a6aw5ugS7jl0QeOdcySweIdG1dPv6uxgdZLFm3cteIsz8KTf09beBZVX8mNpxtL/JnDJmLFeYLMxZjawOXwOcCkwO/xbDnzNjpj5sZ8pqcaLoFG/zyiq7odMYsMjA8zv28IZvZuY37fF25S7pS97iL7OW5gx7iDjBGaMO0hf5y0sfdlDhbx/3HnKvbNUFizOwNVQGk1DpW+M+Xfgt2OGrwRuDR/fCiyqG7/NBDwIdIvINEuy5sLqocWR/UFXDy12JJHjfp8lI20vVZdc13nnKL85wEQZ4rrOO3N/70bnadHcHrb2LuDpvsv9sH4tZgipoTSarD7904wx+8LHzwGnhY97gPqw/t5w7AREZLmI9ItI/4EDJ2YOFEVqf2YBOO33WTJc+8nTzDImDj6Xavw4FnzbRZ0na7MuiwXo1FAaTcvZO8YYIyKpazkYY9YB6yAow9CqHFnJ5M/MGaf9PkuG1al7htaHqbJxsqQ6WvJtF+HisJqdZDFDqKh03LKQVenvF5Fpxph9ofvm+XB8AJhZt92McMxbrF8QlnL+VbE3h7Vc6wzKNXXrxSyKzFKFyEbnycb6AautKC23INTf05/IqvQ3AkuBvvD/3XXjHxORO4C/AA7XuYG8xdoF4WnOf5WxlmudQbmmtp6zKDJLvu2k82TLQrc+m8g7Q6hNaSZl89vAhcAUEdkLfI5A2a8XkQ8DvwZq38wPCNI19xCkbH4wB5n9Ret2F461mVoG5ZpplpFWkVla/Zp0nub3bbFiobfrCtdG+LYKu6HSN8a8J+aliyK2NcDVrQpVWspYk6QCWJmpNVKuEW67FQvn57+i07JvO+o82bLQba9w9U1ZZmHDIwOsuGsHwyNB2HLg0CAr7toBuFuFrStybaIt78pLUuvDmFLCizq25p9JVcDq1+ndXVwx7gEemPAJnjrpvTww4RNcMe6B1Ba6zcyyMqXiJnHjPbuPK/wawyOGG+/Z7UgibaJiF8cdiKpgGTklLgifpclIiXh44z8xZ9tn6apbQzBoJrDr3M/zpis+4kSmqjS1mdUbXckT4Fd9l7d8/CxNVLTgmk0sZxykwddiXqUizt+e5LazWKE195t2jKxvevIfYMyisS4ZCsZxo/R1FW1+qNK3jaOMA6vpcspo4vz9XZPis7Ug35z/tCRllnkYi6pKULi7q5NDg8OR465Qn35FUMsoR+L8/RCdrfXD61O3E8x9xWxSZpmHsaiqrKJdecUb6Bw3unRj5zhh5RVvcCSRKv3KoPVFciQumDr4QvT2g79NXSws95t2kjWfFMR2RFXKjSya28PNi88e9TluXny23ymbSjnwtiFEybqSxRLlttu8Kr55eBS2c/7TkJSS6jAWlURVVtH69jnU0q8IXlpGMamOWZtheEechdw1OXr7BHdJ7u6MRtb8WUuCTKSVh4L/Zbwx+4qlZjC2UEu/QvhmUVR+hXKchQypF1TlXhTMU2u+8nhYmkWVfhkpi8vEw6yQJDKlTCZla/n2HWktm+Lx0PBRpV82PLQcYrFUN6YIrKdMplSwus6ionho+KhPv2x42Mg9Fg+zQuJw3YzF9fsrOeFhOqwq/bLhoeUQSwF1Y2zhep2D6/dXcsJDw0fdO2WjRC4ToDR+ZNcrQKd3d3Hu7+7juvHrmS4HedZMYc3RJWxz2LZTsYCHAXRV+p4SG1S0WGq3bORZm8b1OocvnfkEc7bdcrzo2Qw5yE2dt7DrzFlAeQqMKRF4Zvio0veQ5KCef5ZDEeQd6HTdR9XHomdKNdHSyh5SlbKyNvH1nFibfazsBqJ+ixIsmFKUCLKUVtZArodoUO9EfDwnVht9eJjloVQTVfoe0qh42oZHBpjft4Uzejcxv29L6boJZcHHgnJW0yw9zPJQqokqfQ9JqsNSlTZyafGx1K7V2UeJ0luVcqOBXA9JCirO79vitFmKq5aMrgOtUVhP8/Qsy0OpJqr0PSWueJpL33aWDBqbNwnfCsq5TvNUlCyoe6dkuPRtp/VhV90V5WU5a0VpgFr6JcOldZl2ltEOfXt9m30URlkqvSonoEq/ZLj0baf1YfuYZlkUrmIfhVCmSq/KCajSLyGurMu0s4ykm0SVlWLlyyR7WCNeaZ6WfPoi8isReVREtotIfzg2WUTuE5Enwv+T7IiquCatDzsuzfItr59aaV9/5cskl6nSq3ICNiz9txhjDtY97wU2G2P6RKQ3fH69hfcpBVW2YCHdLCPOFVV1X79tt1YR11Sq9yhbpVdlFHm4d64ELgwf3wr8hDZR+pWf1mcg6iZx7Z3bI7etiq/fZv5+EddU6ve46AaO3v1xxo/88fjQ0Y6TGa+rh0tBqymbBvg3EdkmIsvDsdOMMfvCx88Bp0XtKCLLRaRfRPoPHDjQohh+UPlpvSV8LKlgE5urh4u4plKn4o7Mp3d4GXuPTeGYEfYem0Lv8DI2jMy3JpOSH61a+hcYYwZE5JXAfSLyy/oXjTFGRCLLeBpj1gHrIKiy2aIcXtDO2SppqPqiJpsZVkVcU1lScQeG3sxdvHnU+E8r4p6rOi0pfWPMQPj/eRH5HnAesF9Ephlj9onINOB5C3KWAtfdl8qCjyUVbGMrw6qIa0pTcduLzO4dEfkzEXl57THwVmAXsBFYGm62FLi7VSHLgo9FwZyzcz2snRPUi187J3hOoBS39i7g6b7L2dq7oFIK3yZFXFNp36Pq7rmq04qlfxrwPRGpHed2Y8yPRORhYL2IfBj4NdA2ibvtYMGmQhfxtEwR11Ta96i6e67qaOcsJT/WzolJ7ZsJ1+4qXh7FGlVPTS4LWTpn6YpcJT90EU9laduaQxVAq2wq+aEtABXFO1TpK/mhLQAVxTtU6Sv5oS0AFcU71Kev5Iu2AFQUr1BLX1EUpY1Qpa8oitJGqNJXFEVpI1TpK4qitBGq9BVFUdoIL8owiMgBgjo9RTEFONhwKz8oi6xlkRNU1rwoi6xlkRMay/qfjDFT0xzQC6VfNCLSn7ZehSvKImtZ5ASVNS/KImtZ5IR8ZFX3jqIoShuhSl9RFKWNaFelv861ACkoi6xlkRNU1rwoi6xlkRNykLUtffqKoijtSrta+oqiKG2JKn1FUZQ2olJKX0Qmi8h9IvJE+H9SzHY/EpFDIvL9MeNniMjPRGSPiNwpIhPC8ZPC53vC12cVJOfScJsnRGRpOPZyEdle93dQRL4UvvYBETlQ99qyVuRsVdZw/Cci8nidTK8Mx62e01ZlFZGJIrJJRH4pIrtFpK9ueyvnVUQuCc/FHhHpjXg99pyIyKfD8cdFZGGzx8xKVllF5GIR2SYij4b/F9TtE3ktOJR1logM1snz9bp9zg0/wx4R+YpI0AzckZx/PeY3f0xEzglfS39OjTGV+QPWAL3h417gppjtLgLeDnx/zPh64Krw8deB/xE+/ijw9fDxVcCdecsJTAaeCv9PCh9PithuG/CX4eMPAP9Y9DlNkhX4CTAvYh+r57RVWYGJwFvCbSYA/wFcauu8Ah3Ak8Crw+PvAM5s5pwAZ4bbnwScER6no5ljOpB1LjA9fDwHGKjbJ/JacCjrLGBXzHEfAs4HBPhh7VpwIeeYbd4IPNnKOa2UpQ9cCdwaPr4VWBS1kTFmM/D7+rHwTr4AuCti//rj3gVc1OKdvxk5FwL3GWN+a4x5AbgPuGSMzK8FXkmgoPLCiqwNjmvjnLYkqzHmiDHmfgBjzBDwc8BmX8fzgD3GmKfC498Ryhsnf/05uRK4wxjzkjHmaWBPeLxmjlmorMaYR4wxz4bju4EuETnJgkzWZY07oIhMA04xxjxoAs16GzG6xIGc7wn3zUzVlP5pxph94ePngNNS7PsK4JAx5mj4fC9Q6/zcAzwDEL5+ONw+TzmPv2eEPDVq1kB9CtY7RWSniNwlIjNbkNGmrP8rnHr+Xd1FbPuc2pIVEekmmAlurhtu9bw2833GnZO4fZs5ZhZakbWedwI/N8a8VDcWdS24lPUMEXlERP6PiPzXuu33Njhm0XLWeDfw7TFjqc5p6TpniciPgVdFvPSZ+ifGGCMizvJRC5LzKuB9dc/vAb5tjHlJRD5CYDUsiNyzOFn/2hgzICIvB74TyntbymMcJ+/zKiLjCX5UXzHGPBUOZzqv7YyIvAG4CXhr3bDVa8EC+4DTjTG/EZFzgQ2h3F4iIn8BHDHG7KobTn1OS6f0jTH/Le41EdkvItOMMfvCKdrzKQ79G6BbRMaHd9kZwED42gAwE9gbKoVTw+3zlHMAuLDu+QwC/13tGGcD440x2+res16mWwh83A3JU1ZjzED4//cicjvBNPc2MpzTvGUNWQc8YYz5Ut17ZjqvEe9bP0Oov77GbjP2nCTt2+iYWWhFVkRkBvA94P3GmCdrOyRcC05kDWfIL4UybRORJ4HXhtvXu/ZsnNeWzmnIVYyx8rOc06q5dzYCtcyRpcDdze4YXgD3A++K2L/+uO8CtoxxqeQh573AW0VkkgRZKG8Nx2q8hzEXQKjoalwB/KIFGVuWVUTGi8iUULZO4G1AzUqxfU5bkjWU8fMEP7Rr6newdF4fBmZLkCE2geAHvDFB/vpzshG4KszuOAOYTRBobOaYWcgsa+ga20QQUN9a27jBteBK1qki0hHK9GqC8/pU6CL8nYicH7pL3k8KXWJbzlC+ccAS6vz5mc9pmqiv738E/q/NwBPAj4HJ4fg84Ja67f4DOAAMEvjWFobjryb4Me0B/jdwUjh+cvh8T/j6qwuS80Phe+4BPjjmGE8Brx8ztpogeLaD4Ab2+lbkbFVW4M8Isot2hnJ9GejI45xakHUGYAgU+vbwb5nN8wpcBvxfgiyOz4Rjq4ArGp0TAvfVk8Dj1GWSRB3T0m8pk6zAZ4E/1J3D7QTJBrHXgkNZ3xnKsp0gcP/2umPOI1CgTwL/SFi9wIWc4WsXAg+OOV6mc6plGBRFUdqIqrl3FEVRlARU6SuKorQRqvQVRVHaCFX6iqIobYQqfUVRlDZClb6iKEoboUpfURSljfj/Ze6CqNKOQJ0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_test[:, 0], y_test)\n",
    "plt.scatter(X_test[:, 0], prediction)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
